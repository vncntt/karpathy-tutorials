{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  1849k      0 --:--:-- --:--:-- --:--:-- 1852k\n"
     ]
    }
   ],
   "source": [
    "!curl -o input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 51, 39, 53, 1, 58, 43, 57, 58]\n",
      "['l', 'm', 'a', 'o', ' ', 't', 'e', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: [itos[c] for c in l]\n",
    "decode = lambda l: ([itos[c] for c in l]) if isinstance(l, list) else itos[l]\n",
    "print(encode('lmao test'))\n",
    "print(decode(encode('lmao test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text),dtype = torch.long)\n",
    "print(data.shape,data.dtype)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(text))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:block_size][:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', '!', '$']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model is using tensor([18]) to predict 47\n",
      "the model is using ['F'] to predict i\n",
      "the model is using tensor([18, 47]) to predict 56\n",
      "the model is using ['F', 'i'] to predict r\n",
      "the model is using tensor([18, 47, 56]) to predict 57\n",
      "the model is using ['F', 'i', 'r'] to predict s\n",
      "the model is using tensor([18, 47, 56, 57]) to predict 58\n",
      "the model is using ['F', 'i', 'r', 's'] to predict t\n",
      "the model is using tensor([18, 47, 56, 57, 58]) to predict 1\n",
      "the model is using ['F', 'i', 'r', 's', 't'] to predict  \n",
      "the model is using tensor([18, 47, 56, 57, 58,  1]) to predict 15\n",
      "the model is using ['F', 'i', 'r', 's', 't', ' '] to predict C\n",
      "the model is using tensor([18, 47, 56, 57, 58,  1, 15]) to predict 47\n",
      "the model is using ['F', 'i', 'r', 's', 't', ' ', 'C'] to predict i\n",
      "the model is using tensor([18, 47, 56, 57, 58,  1, 15, 47]) to predict 58\n",
      "the model is using ['F', 'i', 'r', 's', 't', ' ', 'C', 'i'] to predict t\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"the model is using {context} to predict {target}\")\n",
    "    print(f\"the model is using {decode(context.tolist())} to predict {decode(target.tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets: \n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    ix = torch.randint(len(train_data)-block_size,(batch_size,)) #the (block_size,) at the end specifies the number of indicies you want\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "xb,yb = get_batch(\"train\")\n",
    "print('inputs:')\n",
    "print(xb)\n",
    "print('targets: ')\n",
    "print(yb)\n",
    "\n",
    "for x in range(batch_size):\n",
    "    for i in range(block_size-1):\n",
    "        y=0\n",
    "        #print(f\"the context is {xb[x][:i+1]} while the target is {yb[x][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        logits = self.token_embedding_table(idx) # shape of (batch_size,block_size,channel_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C) \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits,loss = self(idx)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            idx_next = torch.multinomial(probs,num_samples=1)\n",
    "            idx = torch.cat((idx,idx_next),dim=1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits,loss = m(xb,yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1,1),dtype=torch.long)\n",
    "print(''.join(decode(m.generate(idx,max_new_tokens=100)[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincentcheng/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(),lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.536130905151367\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(5000):\n",
    "    xb,yb = get_batch('train')\n",
    "    logits,loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "CHNCKIViver HelozR'd jemiok ft hat fo is -mZARSure, Yje'd ureckha;\n",
      "ENCEngiAs! smiTId\n",
      "W:\n",
      "CE:Pich toto ito,'r. alyy f?\n",
      "E-b, zoll terat;&suck:\n",
      "ThgCA:\n",
      "POxcotDUCENC:\n",
      "Ramitr,\n",
      "CHo IEir -kzowakow Chearouy ino in usate't we cksw,\n",
      "JzPY:\n",
      "Sof m Vbs, hatarakis,bereFotomampure,,\n",
      "W:CIN wlflin: ay ced isordwhau'TI w!AUCUNSome! b!\n",
      "nfry andilk!an!\n",
      "DITh\n",
      "If iloinoth hithcot; e zCAr,\n",
      "june, thes aithak;E:\n",
      "\n",
      "Sen ing ve ce athly wnd hrt ve teogs se.\n",
      "VOUMpbe havefulpimngUFLUGott and:\n",
      "ARIUSa-PHEENV\n",
      "PE:\n",
      "Ap arotegnYBupre\n"
     ]
    }
   ],
   "source": [
    "print(''.join(decode(m.generate(idx,max_new_tokens=500)[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want the xth column of `c` to be the average of the first x rows of b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 5.],\n",
      "        [6., 3.],\n",
      "        [7., 4.]])\n",
      "tensor([[15., 12.],\n",
      "        [15., 12.],\n",
      "        [15., 12.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3,3)\n",
    "b = torch.randint(1,10,(3,2)).float()\n",
    "c = a@b\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[6., 5.],\n",
      "        [6., 1.],\n",
      "        [7., 8.]])\n",
      "tensor([[ 6.,  5.],\n",
      "        [12.,  6.],\n",
      "        [19., 14.]])\n"
     ]
    }
   ],
   "source": [
    "#This doesn't give the AVERAGE of the first x rows of `b` but rather it gives the sum. \n",
    "# to add the average, just change a to divide each triangle of ones by the row it's in.\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "b = torch.randint(1,10,(3,2)).float()\n",
    "c = a@b\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[6., 6.],\n",
      "        [5., 3.],\n",
      "        [7., 1.]])\n",
      "tensor([[6.0000, 6.0000],\n",
      "        [5.5000, 4.5000],\n",
      "        [6.0000, 3.3333]])\n"
     ]
    }
   ],
   "source": [
    "# now each row of c is the average of the rows above it\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a/torch.sum(a,1,keepdim=True)\n",
    "b = torch.randint(1,10,(3,2)).float()\n",
    "c = a@b\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "#this is a from above\n",
    "wei = torch.tril(torch.ones(T,T)) \n",
    "wei = wei / torch.sum(wei,1,keepdim=True)\n",
    "print(wei)\n",
    "#wait so the weights in these matrix multiplies are triangular???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (T,T) x (B,T,C) which pytorch changes to (B,T,T) x (B,T,C) --> (B,T,C). \n",
    "# this is some weird einsum matrix multiply thing that i dont understand rn\n",
    "xbow2 = wei @ x \n",
    "#print(xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros(T,T)\n",
    "wei = wei.masked_fill(tril==0,float('-inf'))\n",
    "wei = F.softmax(wei,dim=-1)\n",
    "xbow3 = wei@x\n",
    "torch.allclose(xbow2,xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do a weighted aggregation of previous elements by using matrix multiplication of a lower traingular fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 4: Self-attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "#single head of self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C,head_size,bias=False)\n",
    "query = nn.Linear(C,head_size,bias=False)\n",
    "value = nn.Linear(C,head_size,bias=False)\n",
    "k = key(x) # (B,T,16)\n",
    "q = query(x) # (B,T,16)\n",
    "wei = k @ q.transpose(-2,-1) # (B,T,16) @ (B,16,T) -> (B,T,T)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros(T,T)\n",
    "wei = wei.masked_fill(tril==0,float('-inf')) #without this, each token would attend to every other token, even ones in the future\n",
    "wei = F.softmax(wei,dim=-1) #normalize\n",
    "\n",
    "v = value(x)\n",
    "out = wei@v\n",
    "\n",
    "wei #each element in wei is the attention score between the token at that index and every other token \n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attention is just a commmunication mechanism. In LMs, the communication is each token attends to all the previous ones. But attention can work on any directed graph.\n",
    "- there is no notion of space and position embedded in the transformer. that's why you need to add positional embeddings\n",
    "- above is a \"decoder block\" of the transformer. if you want all tokens to attend to each other like you would with sentiment analysis (you want to let the whole sentence talk to each other to get an output), you would add an encoder block\n",
    "-  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without the multiplying by 1/sqrt(headsize):\n",
      "tensor(1.0632)\n",
      "tensor(0.9891)\n",
      "tensor(15.6088)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = k @ q.transpose(-2,-1) #* head_size**-0.5\n",
    "print(\"Without the multiplying by 1/sqrt(headsize):\")\n",
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the multiplying by 1/sqrt(headsize):\n",
      "tensor(1.0104)\n",
      "tensor(1.0204)\n",
      "tensor(1.1053)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = k @ q.transpose(-2,-1) * head_size**-0.5\n",
    "print(\"With the multiplying by 1/sqrt(headsize):\")\n",
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
